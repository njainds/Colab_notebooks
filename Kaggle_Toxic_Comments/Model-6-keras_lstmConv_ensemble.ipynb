{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hello, Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njainds/Colab_notebooks/blob/master/Kaggle_Toxic_Comments_Feb23/Model-6-keras_lstmConv_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oYZkU7ZN3CL0",
        "outputId": "47c8e6ea-ef4f-4f2b-f625-4d1bfea69383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Name of GPU : {}\".format(torch.cuda.get_device_name(0)))\n",
        "print(\"# of GPU : {}\".format(torch.cuda.device_count()))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name of GPU : Tesla K80\n",
            "# of GPU : 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BAFlm8xPHpyc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import re\n",
        "import torch\n",
        "from torchtext import data\n",
        "import spacy\n",
        "from tqdm import tqdm_notebook, tnrange\n",
        "from tqdm.auto import tqdm\n",
        "import zipfile\n",
        "import gzip\n",
        "import keras.backend as K\n",
        "\n",
        "tqdm.pandas(desc='Progress')\n",
        "from collections import Counter\n",
        "from textblob import TextBlob\n",
        "from nltk import word_tokenize\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.callbacks import Callback\n",
        "from keras.optimizers import Optimizer\n",
        "from keras import backend as K, initializers, regularizers, constraints\n",
        "from keras.regularizers import l2\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "from keras.layers import K, Activation\n",
        "from keras.engine import Layer\n",
        "from keras.layers import PReLU, Conv1D, Conv2D, Input, SpatialDropout1D, Embedding, Dense, Activation, Bidirectional, LSTM,CuDNNLSTM, CuDNNGRU, GlobalMaxPooling1D,GlobalAveragePooling1D, concatenate, Dropout\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam, RMSprop, Nadam\n",
        "from keras.models import Model\n",
        "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "import os\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "\n",
        "# cross validation and metrics\n",
        "from  sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection,linear_model,metrics\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from unidecode import unidecode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D-lZ41oqDYTC",
        "colab_type": "code",
        "outputId": "0b8ef4ee-d6ab-4dc0-fcd1-f90b60304602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive/kaggle_toxic_comments\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "best_model.hdf5\t\t\t   test.csv\n",
            "ext_fasttextwiki_embeddings.npy    test_features.npy\n",
            "ext_glove_embeddings.npy\t   test_labels.csv\n",
            "ext_lexvec_embeddings.npy\t   toxic_lookup_en_wiki_fasttext_300d.npy\n",
            "features.npy\t\t\t   train.csv\n",
            "inputed_lexvec_embeddings.npy\t   word_index.npy\n",
            "local_fasttextwiki_embeddings.npy  x_test.npy\n",
            "Model-runs\t\t\t   x_train.npy\n",
            "sample_submission.csv\t\t   y_train.npy\n",
            "submission_baseline_lr.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ETSksPDYFps2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#configs\n",
        "embed_size = 300 # how big is each word vector\n",
        "max_features = 120000 # how many unique words to use (i.e num rows in embedding vector) ##sum([word[1] for word in vocab[:120000]])=92%##\n",
        "maxlen = 250 # max number of words in a question to use ## train.num_words.quantile(q=0.96, interpolation='nearest')=258 ##\n",
        "batch_size = 512 # how many samples to process at once\n",
        "n_epochs = 5 # how many times to iterate over all samples\n",
        "n_splits = 5 # Number of K-fold Splits\n",
        "\n",
        "SEED = 2019\n",
        "def seed_everything(seed=2019):\n",
        "    random.seed(seed)\n",
        "    \n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "seed_everything()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E2hyLLFNIHoX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/My Drive/kaggle_toxic_comments/\")\n",
        "x_train = np.load(\"x_train.npy\")\n",
        "x_test = np.load(\"x_test.npy\")\n",
        "y_train = np.load(\"y_train.npy\")\n",
        "features = np.load(\"features.npy\")\n",
        "test_features = np.load(\"test_features.npy\")\n",
        "word_index = np.load(\"word_index.npy\").item()\n",
        "ext_lexvec_embeddings = np.load(\"ext_lexvec_embeddings.npy\")\n",
        "ext_glove_embeddings = np.load(\"ext_glove_embeddings.npy\")\n",
        "ext_fasttextwiki_embeddings = np.load(\"ext_fasttextwiki_embeddings.npy\")\n",
        "imputed_lexvec_embeddings = np.load(\"/content/drive/My Drive/kaggle_toxic_comments/inputed_lexvec_embeddings.npy\")\n",
        "#local_fasttextwiki_embeddings = np.load(\"local_fasttextwiki_embeddings.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8jwc1rMTeyDC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train simple LSTM using different embeddings\n",
        "# Top compeitions- QIQC and Toxic comments\n",
        "# ensembles (stacking and blending)\n",
        "# Pretrained models- ULMFit and BERT\n",
        "# Gensim and fasttext text classification\n",
        "# Other SOTAs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wdi_9dQMkT2G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##Model-6: https://github.com/mattmotoki/toxic-comment-classification/blob/master/code/modeling/Refine%20RNN%20Version%201.ipynb\n",
        "#Prelu\n",
        "#Nadam\n",
        "#Attention with Sequence\n",
        "#Multiple seeds emsemble\n",
        "#multiple embeddings ensemble"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eIUW0jz1Wh0F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RxiqwQ77Tivr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RocAucEvaluation(Callback):\n",
        "    def __init__(self, validation_data=(), interval=1):\n",
        "        super(Callback, self).__init__()\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "            score = roc_auc_score(self.y_val, y_pred)\n",
        "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "class CyclicLR(Callback):\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=4590., mode='triangular',gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):     \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v) \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "        \n",
        "def squash(x, axis=-1):\n",
        "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
        "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
        "    return x / scale\n",
        "\n",
        "# A Capsule Implement with Pure Keras\n",
        "class Capsule(Layer):\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
        "                 activation='default', **kwargs):\n",
        "        super(Capsule, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.kernel_size = kernel_size\n",
        "        self.share_weights = share_weights\n",
        "        if activation == 'default':\n",
        "            self.activation = squash\n",
        "        else:\n",
        "            self.activation = Activation(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(Capsule, self).build(input_shape)\n",
        "        input_dim_capsule = input_shape[-1]\n",
        "        if self.share_weights:\n",
        "            self.W = self.add_weight(name='capsule_kernel',\n",
        "                                     shape=(1, input_dim_capsule,\n",
        "                                            self.num_capsule * self.dim_capsule),\n",
        "                                     # shape=self.kernel_size,\n",
        "                                     initializer='glorot_uniform',\n",
        "                                     trainable=True)\n",
        "        else:\n",
        "            input_num_capsule = input_shape[-2]\n",
        "            self.W = self.add_weight(name='capsule_kernel',\n",
        "                                     shape=(input_num_capsule,\n",
        "                                            input_dim_capsule,\n",
        "                                            self.num_capsule * self.dim_capsule),\n",
        "                                     initializer='glorot_uniform',\n",
        "                                     trainable=True)\n",
        "\n",
        "    def call(self, u_vecs):\n",
        "        if self.share_weights:\n",
        "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
        "        else:\n",
        "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
        "\n",
        "        batch_size = K.shape(u_vecs)[0]\n",
        "        input_num_capsule = K.shape(u_vecs)[1]\n",
        "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
        "                                            self.num_capsule, self.dim_capsule))\n",
        "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
        "\n",
        "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
        "        for i in range(self.routings):\n",
        "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
        "            c = K.softmax(b)\n",
        "            c = K.permute_dimensions(c, (0, 2, 1))\n",
        "            b = K.permute_dimensions(b, (0, 2, 1))\n",
        "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
        "            if i < self.routings - 1:\n",
        "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, self.num_capsule, self.dim_capsule)\n",
        "\n",
        "\n",
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    Wrapper for dot product operation, in order to be compatible with both\n",
        "    Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n",
        "    \n",
        "\n",
        "class AttentionWithContext(Layer):\n",
        "    \"\"\"\n",
        "    Attention operation, with a context/query vector, for temporal data.\n",
        "    Supports Masking.\n",
        "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
        "    \"Hierarchical Attention Networks for Document Classification\"\n",
        "    by using a context vector to assist the attention\n",
        "    # Input shape\n",
        "        3D tensor with shape: `(samples, steps, features)`.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(samples, features)`.\n",
        "    How to use:\n",
        "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "    The dimensions are inferred based on the output shape of the RNN.\n",
        "    Note: The layer has been tested with Keras 2.0.6\n",
        "    Example:\n",
        "        model.add(LSTM(64, return_sequences=True))\n",
        "        model.add(AttentionWithContext())\n",
        "        # next add a Dense layer (for classification/regression) or whatever...\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.u_constraint = constraints.get(u_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[-1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "\n",
        "        self.u = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint)\n",
        "\n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        uit = dot_product(x, self.W)\n",
        "\n",
        "        if self.bias:\n",
        "            uit += self.b\n",
        "\n",
        "        uit = K.tanh(uit)\n",
        "        ait = dot_product(uit, self.u)\n",
        "\n",
        "        a = K.exp(ait)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YEPJ623BlR17",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model(embed_name, n_recurrent, n_filters, l2_penalty=0.0001):\n",
        "    K.clear_session()\n",
        "    embed_matrix = np.load(embed_name)\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    emb   = Embedding(max_features, embed_size, weights=[embed_matrix], trainable=False)(inp)\n",
        "    x   = SpatialDropout1D(rate=0.2)(emb)\n",
        "    x   = Bidirectional(CuDNNGRU(n_recurrent, return_sequences=True, kernel_regularizer=l2(l2_penalty),recurrent_regularizer=l2(l2_penalty)))(x)\n",
        "    x   = Conv1D(64, kernel_size=1, padding='valid', kernel_initializer='glorot_normal')(x)\n",
        "    x = PReLU()(x)\n",
        "    x_att = AttentionWithContext()(x)\n",
        "    x_max   = GlobalMaxPooling1D()(x)\n",
        "    x_avg   = GlobalAveragePooling1D()(x)\n",
        "    \n",
        "    x1   = SpatialDropout1D(rate=0.2)(emb)\n",
        "    x1   = Bidirectional(CuDNNGRU(n_recurrent, return_sequences=True, kernel_regularizer=l2(l2_penalty),recurrent_regularizer=l2(l2_penalty)))(x1)\n",
        "    x1   = Conv1D(2*64, kernel_size=1, padding='valid', kernel_initializer='glorot_normal')(x1)\n",
        "    x1 = PReLU()(x1)\n",
        "    x1_att = AttentionWithContext()(x1)\n",
        "    x1_max   = GlobalMaxPooling1D()(x1)\n",
        "    x1_avg   = GlobalAveragePooling1D()(x1)\n",
        "    \n",
        "    c   = concatenate([x_max,x_avg,x_att,x1_max,x1_avg,x1_att])\n",
        "    out = Dense(6, activation='sigmoid')(c)\n",
        "    model=Model(inputs=inp,outputs=out)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def pred_model(emb, n_splits, seed):\n",
        "  \n",
        "    kfold = KFold(n_splits=n_splits, random_state=seed, shuffle=True)\n",
        "    y_test = np.zeros((x_test.shape[0], 6))\n",
        "    for i, (train_index, valid_index) in enumerate(kfold.split(x_train)):\n",
        "        X_train, X_val, Y_train, Y_val = x_train[train_index], x_train[valid_index], y_train[train_index], y_train[valid_index]\n",
        "        file_path = \"best_model.hdf5\"\n",
        "        check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,save_best_only = True, mode = \"min\")\n",
        "        ra_val = RocAucEvaluation(validation_data=(X_val, Y_val), interval = 1)\n",
        "        early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)\n",
        "        model = get_model(n_recurrent=50, n_filters=50, embed_name=emb, l2_penalty=0.0001)\n",
        "        print(\"### Model: {} with seed: {}  for fold no. {} ####\".format(emb, seed, i))\n",
        "        clr = CyclicLR(base_lr=1e-6, max_lr=1e-3,step_size=2524., mode='exp_range',gamma=0.99994)\n",
        "        model.fit(X_train, Y_train, batch_size = 128, epochs = 5, validation_data = (X_val, Y_val), verbose = 1, callbacks = [clr, ra_val, check_point, early_stop])\n",
        "        y_pred = model.predict([X_val], batch_size=batch_size, verbose=2)\n",
        "        y_test += model.predict([x_test], batch_size=1024, verbose=1)/5\n",
        "        os.remove(file_path)\n",
        "    return y_test\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vAegUwOJ1Juw",
        "colab_type": "code",
        "outputId": "9c8d0b65-e64a-41a9-b94f-f3a922a22eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1309
        }
      },
      "cell_type": "code",
      "source": [
        "for emb in ['ext_lexvec_embeddings.npy', 'ext_fasttext_wiki_embeddings.npy', 'ext_glove_embeddings.npy', 'imputed_lexvec_embeddings.npy']:    \n",
        "    for seed in [0, 1, 2]:\n",
        "        pred = pred_model(emb, n_splits=10, seed=seed)\n",
        "        pred += pred/12"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Model: ext_lexvec_embeddings.npy with seed: 0  for fold no. 0 ####\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/5\n",
            "143613/143613 [==============================] - 246s 2ms/step - loss: 0.2379 - acc: 0.9457 - val_loss: 0.0917 - val_acc: 0.9800\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.939959\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.09166, saving model to best_model.hdf5\n",
            "Epoch 2/5\n",
            "143613/143613 [==============================] - 244s 2ms/step - loss: 0.0748 - acc: 0.9803 - val_loss: 0.0621 - val_acc: 0.9821\n",
            "\n",
            " ROC-AUC - epoch: 2 - score: 0.969875\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.09166 to 0.06206, saving model to best_model.hdf5\n",
            "Epoch 3/5\n",
            "143613/143613 [==============================] - 244s 2ms/step - loss: 0.0584 - acc: 0.9817 - val_loss: 0.0523 - val_acc: 0.9830\n",
            "\n",
            " ROC-AUC - epoch: 3 - score: 0.977145\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.06206 to 0.05230, saving model to best_model.hdf5\n",
            "Epoch 4/5\n",
            "143613/143613 [==============================] - 245s 2ms/step - loss: 0.0516 - acc: 0.9827 - val_loss: 0.0486 - val_acc: 0.9833\n",
            "\n",
            " ROC-AUC - epoch: 4 - score: 0.979481\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.05230 to 0.04864, saving model to best_model.hdf5\n",
            "Epoch 5/5\n",
            "143613/143613 [==============================] - 244s 2ms/step - loss: 0.0488 - acc: 0.9833 - val_loss: 0.0476 - val_acc: 0.9835\n",
            "\n",
            " ROC-AUC - epoch: 5 - score: 0.979984\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.04864 to 0.04758, saving model to best_model.hdf5\n",
            "153164/153164 [==============================] - 38s 247us/step\n",
            "### Model: ext_lexvec_embeddings.npy with seed: 0  for fold no. 1 ####\n",
            "Train on 143614 samples, validate on 15957 samples\n",
            "Epoch 1/5\n",
            "143614/143614 [==============================] - 246s 2ms/step - loss: 0.2235 - acc: 0.9576 - val_loss: 0.1024 - val_acc: 0.9762\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.948258\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.10240, saving model to best_model.hdf5\n",
            "Epoch 2/5\n",
            "143614/143614 [==============================] - 245s 2ms/step - loss: 0.0768 - acc: 0.9802 - val_loss: 0.0661 - val_acc: 0.9805\n",
            "\n",
            " ROC-AUC - epoch: 2 - score: 0.969911\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.10240 to 0.06613, saving model to best_model.hdf5\n",
            "Epoch 3/5\n",
            "143614/143614 [==============================] - 244s 2ms/step - loss: 0.0592 - acc: 0.9816 - val_loss: 0.0573 - val_acc: 0.9809\n",
            "\n",
            " ROC-AUC - epoch: 3 - score: 0.976700\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.06613 to 0.05734, saving model to best_model.hdf5\n",
            "Epoch 4/5\n",
            "143614/143614 [==============================] - 244s 2ms/step - loss: 0.0529 - acc: 0.9822 - val_loss: 0.0520 - val_acc: 0.9817\n",
            "\n",
            " ROC-AUC - epoch: 4 - score: 0.979447\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.05734 to 0.05198, saving model to best_model.hdf5\n",
            "Epoch 5/5\n",
            "143614/143614 [==============================] - 245s 2ms/step - loss: 0.0505 - acc: 0.9827 - val_loss: 0.0515 - val_acc: 0.9820\n",
            "\n",
            " ROC-AUC - epoch: 5 - score: 0.979944\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.05198 to 0.05152, saving model to best_model.hdf5\n",
            "153164/153164 [==============================] - 38s 247us/step\n",
            "### Model: ext_lexvec_embeddings.npy with seed: 0  for fold no. 2 ####\n",
            "Train on 143614 samples, validate on 15957 samples\n",
            "Epoch 1/5\n",
            "143614/143614 [==============================] - 246s 2ms/step - loss: 0.2465 - acc: 0.9392 - val_loss: 0.1039 - val_acc: 0.9771\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.862054\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.10391, saving model to best_model.hdf5\n",
            "Epoch 2/5\n",
            "125568/143614 [=========================>....] - ETA: 29s - loss: 0.0792 - acc: 0.9801"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ngNuKxFWn7Xp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv(\"./sample_submission.csv\")\n",
        "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = (pred)\n",
        "submission.to_csv(\"submission_model6_Feb24_auc_988317.csv\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vep6bR7etg4v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0eB1ufDwzxLr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "y_test = np.zeros((x_test.shape[0], 6))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7lPbxkVy07A4",
        "colab_type": "code",
        "outputId": "4152eb3b-5b1e-4623-cb56-ae75f6b54cda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(153164, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "cogJ7mAj26-I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w7096K_ZnsNC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yAbN93Z7nSxg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vcvd_PdAoY--",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jOiL1lHD6Ffu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lIS3AizP9vdR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s7c_QyY9ytW5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
